{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROJECT 2 Data wrangling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims at  gathering assessing and cleaning real world data.The data used was  is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10. The numerators, though? Almost always greater than 10. 11/10, 12/10, 13/10, etc. Why? Because \"they're good dogs Brent.\" WeRateDogs has over 4 million followers and has received international media coverage."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follwing steps were used to achieve the main goal(data wrangling)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Gather data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directly downloaded the WeRateDogs Twitter archive data (twitter_archive_enhanced.csv)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "\n",
    "we_rate_dogs = pd.read_csv('twitter-archive-enhanced.csv',header=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloaded the image predictions programmatically using the requests module\n",
    "\n",
    "response=requests.get('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv')\n",
    "\n",
    "with open('image_prediction.tsv', 'wb') as file:\n",
    "\n",
    "    file.write(response.content)\n",
    "\n",
    "image_predictions = pd.read_csv('image_prediction.tsv', sep=\"\\t\", header=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Used the tweepy api to download additional tweet data and stored it in a txt file\n",
    "\n",
    "The txt file then was then used to extract the additional tweet data using the json module\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import json\n",
    "listt = []\n",
    "##### with open('tweet_json.txt', encoding='utf-8') as file:\n",
    "with open('tweet_json.txt', encoding='utf-8') as file:\n",
    "    for line in file.readlines():\n",
    "        line = json.loads(line)\n",
    "        idd = line['id']\n",
    "        created_at = line['created_at']\n",
    "        retweet_count = line['retweet_count']\n",
    "        favorite_count = line['favorite_count']\n",
    "        listt.append({'tweet_id':idd, 'favorite_count':favorite_count,'retweet_count': retweet_count, 'created_at' : created_at})\n",
    "\n",
    "        \n",
    "additional_tweets =pd.DataFrame(listt, columns=['tweet_id', 'favorite_count', 'retweet_count', 'created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ASSESS DATA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both visual and programmatic assessment was done on all the three data sets to find * quality issues and 2 tidiness issues \n",
    "\n",
    "some of the methods used were"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we_rate_dogs.info()            - image_predictions.head(5)     - additional_tweets.info()\n",
    "\n",
    "-  we_rate_dogs.head()               - image_predictions.info()\n",
    "\n",
    "-  we_rate_dogas.describe()         - image_predictions.describe()\n",
    "\n",
    "- we_rate_dogs.sample()            -additional_tweets.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Quality issues were found."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. missing values in we_rate_dogs columns (in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp, expanded_urls)\n",
    "\n",
    "2. source column in the weratedogs csv has html tags\n",
    "\n",
    "3. Timestamp column in weratedogs has invalid datetime format\n",
    "\n",
    "4. The ID fields, like tweet_id in the three datasets should be objects and not numeric since they are not required to perform any calculations\n",
    "\n",
    "5. Retweets and Favorite Count: retweet_count and favorite_count should be integers, not floats.\n",
    "\n",
    "6. rating_numerator column should be of float data type.The rating denominator is of int data type however it is preffered as float data type\n",
    "\n",
    "7. Rating numerator  and rating denominator have anomalous values (1776, 170). \n",
    "\n",
    "8. In the name column, there are several values that are not dog names, like 'a', 'the', 'such', etc.\n",
    "\n",
    "9. In image predictions csv, columns p1, p2, p3 separates two or three names with an underscore\n",
    "\n",
    "10. Invaid date-time format in additional tweets dataframe\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following tidiness issues were found"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.The 4 different columns doggo, floofer, pupper and puppo, are all relative to the same variable that identifies the stage of dog.\n",
    "\n",
    "2.The melted dataframe have 2 columns with the same attribute\n",
    "\n",
    "3.The merged df1 dataset has null values in some columns.This does not follow the law that each variable forms a column\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Clean Data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After assessing the data , a copy of  each dataset was created to perform cleaning without affecting the original data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following  methods were used "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop columns with missing values more than 50%. Fill missing values in the expanded_urls column with a random url"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#drop columns with missing values more than 50%.\n",
    "columns = [\"in_reply_to_status_id\", \"in_reply_to_user_id\", \"retweeted_status_id\", \"retweeted_status_user_id\", \"retweeted_status_timestamp\"]\n",
    "print(we_rate_dogs_copy.isna().mean().mul(100))\n",
    "we_rate_dogs_copy.drop(columns=columns, axis=1, inplace=True)\n",
    "\n",
    "#Fill missing values in the expanded_urls column with a random url\n",
    "\n",
    "random_url = we_rate_dogs_copy.expanded_urls[0]\n",
    "we_rate_dogs_copy['expanded_urls'].fillna(random_url, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove html tags in the source column to only have the url using the string split method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def slicing(row):\n",
    "    return row[1][:-4]\n",
    "we_rate_dogs_copy['source'] = we_rate_dogs_copy['source'].str.split(\">\", 1).apply(slicing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix the datetime format by removing the unnecessary zeros at the end of each datetime object"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def time_slicing(row):\n",
    "    return row[:-6]\n",
    "we_rate_dogs_copy.timestamp = we_rate_dogs_copy.timestamp.apply(time_slicing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### change the tweet id column datatype of the three datasets from int/float to object data type using astype method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we_rate_dogs_copy[\"tweet_id\"] = we_rate_dogs_copy[\"tweet_id\"].astype(str)\n",
    "\n",
    "image_predictions_copy[\"tweet_id\"] = image_predictions_copy[\"tweet_id\"].astype(str)\n",
    "\n",
    "additional_tweets_copy[\"tweet_id\"] = additional_tweets_copy[\"tweet_id\"].astype(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the astype method we will change the format to int of the columns retweet count and favorite count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def change_int(col):\n",
    "    return col.astype(int)\n",
    "\n",
    "columns =['retweet_count', 'favorite_count']\n",
    "additional_tweets_copy[columns] = additional_tweets_copy[columns].apply(change_int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the rating numerator and rating denominator datatypes to the preferred ones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we_rate_dogs_copy.rating_numerator = we_rate_dogs_copy.rating_numerator.astype(float)\n",
    "we_rate_dogs_copy.rating_denominator = we_rate_dogs_copy.rating_denominator.astype(float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop anomalies in the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "we_rate_dogs_copy = we_rate_dogs_copy.drop(axis=0, index=[979])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing invalid names in the dataset eg 'like', 'such'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#drop duoplicated names\n",
    "we_rate_dogs_copy = we_rate_dogs_copy.drop_duplicates(['name'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#get valid names\n",
    "def get_valid_name(row):\n",
    "    if not row[0].islower():\n",
    "        return row\n",
    "    \n",
    "we_rate_dogs_copy.name = we_rate_dogs_copy.name.apply(get_valid_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#drop rows that have nulls\n",
    "\n",
    "we_rate_dogs_copy = we_rate_dogs_copy.dropna(axis = 0, how ='any')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In image predictions csv, we replace the underscore with an empty space  columns p1, p2, p3 using the str.replace method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "columns = ['p1', 'p2', 'p3']\n",
    "def replace(row):\n",
    "    row.replace(\"_\", \" \")\n",
    "\n",
    "for col in columns:\n",
    "    image_predictions_copy[col] = image_predictions_copy[col].str.replace(\"_\", \" \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### melt columns doggo, floofer, pupper and puppo to a single column named dog_stage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we_rate_dogs_copy = pd.melt(we_rate_dogs_copy, id_vars=['tweet_id', 'timestamp', 'source', 'text', 'expanded_urls',\n",
    "       'rating_numerator', 'rating_denominator', 'name'],\n",
    "                           var_name='dog_stage')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After fixing the quality and tidy issues , I went ahead and merged the datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#merge two datasets(we rate dogs and image predictions)\n",
    "df1 =pd.merge(we_rate_dogs_copy, image_predictions_copy, how=\"left\")\n",
    "\n",
    "#remove nulls\n",
    "df1 =df1.dropna(axis = 0, how ='any')\n",
    "\n",
    "df2 = pd.merge(df1, additional_tweets_copy, how=\"left\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Storing Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The merged data needed to be stored in a csv file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#finally store to our twitter archive master csv\n",
    "\n",
    "df2.to_csv(\"twitter_archive_master.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "063e5440ec48e730499c9292bb9a58f689d36e250db21ac229eedfcbda0885e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
